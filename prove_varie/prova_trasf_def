import torch
from torchvision.transforms import v2 as T
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

def apply_same_transformation_all_channels(original_tensor: torch.Tensor, transform : object = None):
    """
    Take as input a tensor referring to a single patient (all modalities, pre and post nac, all slices)
    and process slice by slice so that the same transformation is applied accross the different channels
    (for example vertical flip).
    Args:
        original_tensor: the tensor referring to a single patient
        transform: the torchvision transformation to be applied
    Returns:
        reconstructed_tensor: a tensor with the same dimension as the original, but with transformations applied.
    """
    num_modalities = original_tensor.shape[0]
    num_slices = original_tensor.shape[1]

    reconstructed_tensor = torch.Tensor()

    for slice in range(num_slices):
        slice_tensor = torch.Tensor()
        for modality in range(num_modalities):
            slice_tensor = torch.cat((slice_tensor, original_tensor[modality][slice].unsqueeze(0)))
        print(f"Slice numero {slice+1}!")
        print("")

        #apply transformation
        slice_tensor = transform(slice_tensor)

        #Riassembla il tensore
        print(f"Shape slice_tensor after transformation: {slice_tensor.shape}")

        divided_modalities = torch.Tensor()
        for i in range(num_modalities):
            modality_i = slice_tensor[i].unsqueeze(0)#.unsqueeze(0) #ok
            divided_modalities = torch.cat((divided_modalities, modality_i)) #ok
        
        #print(f"Modalit√† riassemblate: {modalita_separate}")
        print(f"Shape after divided modalities: {divided_modalities.shape}")
        reconstructed_tensor = torch.cat((reconstructed_tensor, divided_modalities.unsqueeze(0)))

    print("\Original tensor has shape: ", original_tensor.shape)
    reconstructed_tensor = torch.transpose(reconstructed_tensor, 0,1)
    print("\nTransformed tensor has shape: ", reconstructed_tensor.shape)
    assert original_tensor.shape == reconstructed_tensor.shape

    return reconstructed_tensor

transforms = T.Compose([
    #T.RandomResizedCrop(size=(4, 4), antialias=True),
    T.RandomHorizontalFlip(p=1),
    #T.ToDtype(torch.float32, scale=True),
    #T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

original_tensor = torch.randn((8,3,3,224,224))
transformed = apply_same_transformation_all_channels(original_tensor, transforms)

#originali
slice1_mod1 = original_tensor[0][0]
slice1_mod2 = original_tensor[1][0]
slice2_mod1 = original_tensor[0][1]
slice2_mod2 = original_tensor[1][1]
plt.imshow(slice1_mod1.view(slice1_mod1.shape[1], slice1_mod1.shape[2], slice1_mod1.shape[0]))
plt.show()
plt.imshow(slice1_mod2.view(slice1_mod2.shape[1], slice1_mod2.shape[2], slice1_mod2.shape[0]))
plt.show()
plt.imshow(slice2_mod1.view(slice2_mod1.shape[1], slice2_mod1.shape[2], slice2_mod1.shape[0]))
plt.show()
plt.imshow(slice2_mod2.view(slice2_mod2.shape[1], slice2_mod2.shape[2], slice2_mod2.shape[0]))
plt.show()

#transformed
slice1_mod1 = transformed[0][0]
slice1_mod2 = transformed[1][0]
slice2_mod1 = transformed[0][1]
slice2_mod2 = transformed[1][1]
plt.imshow(slice1_mod1.view(slice1_mod1.shape[1], slice1_mod1.shape[2], slice1_mod1.shape[0]))
plt.show()
plt.imshow(slice1_mod2.view(slice1_mod2.shape[1], slice1_mod2.shape[2], slice1_mod2.shape[0]))
plt.show()
plt.imshow(slice2_mod1.view(slice2_mod1.shape[1], slice2_mod1.shape[2], slice2_mod1.shape[0]))
plt.show()
plt.imshow(slice2_mod2.view(slice2_mod2.shape[1], slice2_mod2.shape[2], slice2_mod2.shape[0]))
plt.show()

